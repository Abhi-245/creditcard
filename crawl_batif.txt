"""
recursivecrawl.py
-----------------
Crawls a page (single-depth), extracts text + links only from the specified DOM regions:
  /html/body/div[1]/section[4]
  /html/body/div[1]/section[5]
  /html/body/div[1]/section[6]/div/section

Enhancements:
 - Extracts and saves links found inside those nodes.
 - Crawls each extracted link (depth=1 only) safely.
 - Shows progress bars for main + follow-up pages.
 - Creates a folder `linkbylinkscrap/` and saves per-link scrapes there.
 - Creates a combined file `combinedscrapping.txt` with all content.
"""

import asyncio
import os
import re
from urllib.parse import urldefrag, urljoin
from bs4 import BeautifulSoup
from crawl4ai import (
    AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode,
    MemoryAdaptiveDispatcher
)
from tqdm.asyncio import tqdm_asyncio
from tqdm import tqdm

def safe_filename_from_url(url: str, suffix: str = "", folder: str = "") -> str:
    safe = re.sub(r'[^\w\-_.]', '_', url)
    if suffix:
        safe = f"{safe}_{suffix}"
    if folder:
        os.makedirs(folder, exist_ok=True)
        return os.path.join(folder, f"{safe}.txt")
    return f"{safe}.txt"

def filter_valid_links(links, base_url):
    cleaned = []
    for l in links:
        if not l.startswith("http"):
            continue
        if l.lower().endswith(".pdf"):
            continue
        if "#" in l:
            continue
        if l.rstrip("/") == base_url.rstrip("/"):
            continue
        if "javascript:" in l.lower() or "mailto:" in l.lower():
            continue
        l = l.replace("http://", "https://")
        cleaned.append(l)
    return list(sorted(set(cleaned)))

async def crawl_content_sections(start_urls, max_concurrent=20):
    browser_config = BrowserConfig(headless=True, verbose=False)
    # run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)
    
    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)

    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=max_concurrent
    )

    visited = set()
    def normalize_url(url):
        return urldefrag(url)[0]

    current_urls = [normalize_url(u) for u in start_urls]

    async with AsyncWebCrawler(config=browser_config) as crawler:
        print("\n=== Crawling Main Pages ===")
        urls_to_crawl = [u for u in current_urls if u not in visited]
        if not urls_to_crawl:
            print("No URLs to crawl.")
            return

        results = await crawler.arun_many(
            urls=urls_to_crawl,
            config=run_config,
            dispatcher=dispatcher
        )
        if not results:
            print("No results returned from arun_many.")
            return

        combined_texts = []  # for combinedscrapping.txt

        for result in tqdm(results, desc="Scraping main pages"):
            url = normalize_url(result.url)
            visited.add(url)
            if not result.success:
                print(f"[ERROR] {result.url}: {result.error_message}")
                continue

            print(f"\n[OK] {result.url} | Markdown: {len(result.markdown) if result.markdown else 0} chars")

            html_source = result.html or ""
            links_set = set()
            section_texts = []

            # Extract sections (lxml fallback to BeautifulSoup)
            try:
                import lxml.html as lxml_html
                tree = lxml_html.fromstring(html_source)

                nodes = []
                nodes.extend(tree.xpath('/html/body/div[1]/section[4]'))
                nodes.extend(tree.xpath('/html/body/div[1]/section[5]'))
                nodes.extend(tree.xpath('/html/body/div[1]/section[6]/div/section'))
                if not nodes:
                    nodes = tree.xpath('//div[contains(@class,"content")]') or [tree]

                for node in nodes:
                    text = node.text_content().strip()
                    if text:
                        section_texts.append(text)
                    for a in node.xpath('.//a[@href]'):
                        href = a.get('href')
                        if href and not href.startswith('#') and not href.lower().startswith('javascript:'):
                            links_set.add(urljoin(result.url, href))

            except Exception:
                soup = BeautifulSoup(html_source, "html.parser")
                nodes = []
                sel1 = soup.select('body > div:nth-of-type(1) > section:nth-of-type(4)')
                sel2 = soup.select('body > div:nth-of-type(1) > section:nth-of-type(5)')
                sel3 = soup.select('body > div:nth-of-type(1) > section:nth-of-type(6) > div > section')
                nodes = sel1 + sel2 + sel3 or soup.find_all("div", class_="content") or [soup]

                for node in nodes:
                    text = node.get_text(separator="\n", strip=True)
                    if text:
                        section_texts.append(text)
                    for a in node.find_all("a", href=True):
                        href = a.get("href").strip()
                        if href and not href.startswith('#') and not href.lower().startswith('javascript:'):
                            links_set.add(urljoin(result.url, href))

            # Save main page content
            combined_text = "\n\n".join(section_texts).strip()
            combined_texts.append(f"==== Main Page: {result.url} ====\n{combined_text}\n")
            txt_filename = safe_filename_from_url(result.url, suffix="sections")
            with open(txt_filename, "w", encoding="utf-8") as f:
                f.write("URL: " + result.url + "\n\nLinks:\n")
                f.write("\n".join(sorted(links_set)) if links_set else "(none)")
                f.write("\n\n--- Extracted Section Text ---\n\n")
                f.write(combined_text or "(no content extracted)\n")
            print(f"Saved main page -> {txt_filename}")

            # Filter follow-up links
            valid_links = filter_valid_links(links_set, result.url)
            print(f"Valid links to follow ({len(valid_links)}): {valid_links}")

            # Crawl follow-up links safely with progress
            follow_results = []
            if valid_links:
                try:
                    tmp_results = await crawler.arun_many(
                        urls=valid_links,
                        config=run_config,
                        dispatcher=dispatcher
                    )
                    if tmp_results:
                        follow_results = tmp_results
                    else:
                        print("[WARN] crawler.arun_many returned None for follow-up links.")
                except Exception as e:
                    print(f"[ERROR] Failed crawling follow-up links: {e}")

            # Process follow-up pages
            for sub_result in tqdm(follow_results, desc=f"Scraping links for {url[:30]}..."):
                if not sub_result.success:
                    print(f"[ERROR] {sub_result.url}: {sub_result.error_message}")
                    continue
                safe_url = safe_filename_from_url(sub_result.url, folder="linkbylinkscrap")
                if sub_result.url.lower().endswith(".pdf"):
                    with open(safe_url, "w", encoding="utf-8") as f:
                        f.write(f"PDF link: {sub_result.url}\n")
                    combined_texts.append(f"==== PDF Link: {sub_result.url} ====\n(Saved as PDF reference)\n")
                    print(f"[PDF] Saved PDF reference -> {safe_url}")
                    continue
                soup2 = BeautifulSoup(sub_result.html or "", "html.parser")
                text2 = soup2.get_text(separator="\n", strip=True)
                with open(safe_url, "w", encoding="utf-8") as f:
                    f.write("URL: " + sub_result.url + "\n\n")
                    f.write(text2)
                combined_texts.append(f"==== Linked Page: {sub_result.url} ====\n{text2}\n")
                print(f"Saved linked page -> {safe_url}")

        # Save combined output
        with open("newcombinedscrapping.txt", "w", encoding="utf-8") as f:
            f.write("\n\n".join(combined_texts))
        print("\nSaved combined output to combinedscrapping.txt")


if __name__ == "__main__":
    asyncio.run(crawl_content_sections(
        ["https://www.sbicard.com/en/personal/credit-cards/rewards/cashback-sbi-card.page"],
        max_concurrent=20
    ))
